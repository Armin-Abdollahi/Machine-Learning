{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6etAIM8noq1OMuMTdEQJX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Armin-Abdollahi/Machine-Learning/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers\n",
        "\n",
        "Transformers are deep learning architectures designed for handling sequential data without relying on recurrence, which is commonly used in RNNs. Instead, Transformers use a mechanism called self-attention to process all tokens in the sequence simultaneously, capturing dependencies between tokens regardless of their distance in the sequence. Transformers have become the foundation of many NLP tasks and models, including BERT and GPT.\n",
        "\n",
        "Here’s a basic implementation using TensorFlow for a Transformer model to classify text sequences:"
      ],
      "metadata": {
        "id": "3vRYwfh1KF2K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5acZjFQ0KFWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b21890f-93ca-4572-d6e5-5a9a6f3a77fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# Load and preprocess the IMDB dataset\n",
        "max_features = 10000 # Vocabulary size\n",
        "max_len = 200 # Limit review length to 200 words\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Define a Transformer block\n",
        "class TransformerBlock(layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.ffn = tf.keras.Sequential([\n",
        "        layers.Dense(ff_dim, activation=\"relu\"),\n",
        "        layers.Dense(embed_dim),\n",
        "    ])\n",
        "    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout1 = layers.Dropout(rate)\n",
        "    self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    attn_output = self.att(inputs, inputs)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(inputs + attn_output)\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Define the model with an embedding layer, tensformer block, and output layer\n",
        "embed_dim = 32\n",
        "num_heads = 2\n",
        "ff_dim = 32\n",
        "\n",
        "inputs = layers.Input(shape=(max_len,))\n",
        "embedding_layers = layers.Embedding"
      ]
    }
  ]
}
