{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoJo2fg0nWF3UZF2R5fIRz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Armin-Abdollahi/Machine-Learning/blob/main/Recurrent_Neural_Networks_(RNNs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks (RNNs)\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are neural networks designed for sequential data, such as time series, language, or speech. RNNs have connections that form cycles, allowing them to retain information from previous steps in the sequence. This makes RNNs well-suited for tasks like text generation, language modeling, and time series forecasting. A common variant, Long Short-Term Memory (LSTM), helps to address the issue of long-term dependency.\n",
        "\n",
        "Here’s a simple RNN implementation using TensorFlow and Keras for text classification:"
      ],
      "metadata": {
        "id": "aZfp2UJNASg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# Load and preprocess the IMDB dataset\n",
        "max_features = 10000 # Vocabulary size\n",
        "max_len = 500        # Limit reviews to 500 words\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Define the RNN model\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(max_features, 32, input_length=max_len),\n",
        "    layers.SimpleRNN(32),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "id": "LkhgcNRhFPNZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70b7144-b233-472d-8084-753f6f61a41b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 162ms/step - accuracy: 0.5840 - loss: 0.6545 - val_accuracy: 0.6884 - val_loss: 0.5805\n",
            "Epoch 2/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 151ms/step - accuracy: 0.8106 - loss: 0.4308 - val_accuracy: 0.8342 - val_loss: 0.4063\n",
            "Epoch 3/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 156ms/step - accuracy: 0.8713 - loss: 0.3104 - val_accuracy: 0.6258 - val_loss: 0.6667\n",
            "Epoch 4/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 160ms/step - accuracy: 0.7631 - loss: 0.4904 - val_accuracy: 0.6350 - val_loss: 0.6682\n",
            "Epoch 5/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 154ms/step - accuracy: 0.8911 - loss: 0.2992 - val_accuracy: 0.6508 - val_loss: 0.7309\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.6471 - loss: 0.7319\n",
            "Test accuracy: 0.651199996471405\n"
          ]
        }
      ]
    }
  ]
}