{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdh0bBQi+x/Vw5R6gOwCbC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Armin-Abdollahi/Machine-Learning/blob/main/Ridge_and_Lasso_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridge and Lasso Regression\n",
        "\n",
        "Ridge and Lasso Regression are regularization techniques applied to Linear Regression to prevent overfitting by penalizing large coe8icients:\n",
        "• Ridge Regression adds an L2 penalty (sum of squared coe8icients).\n",
        "• Lasso Regression adds an L1 penalty (sum of absolute values of coe8icients), which can lead to feature selection by shrinking some coe8icients to zero.\n",
        "\n",
        "Here’s a simple implementation using Scikit-Learn for both Ridge and Lasso Regression:"
      ],
      "metadata": {
        "id": "RzcYcEqmSuKw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yFWKmTBSMze",
        "outputId": "844b7190-4229-466e-e1c7-a63f0026942b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Mean Squared Error: 4973271500.612812\n",
            "Lasso Mean Squared Error: 4973274812.251329\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Sample data (e.g., house size vs. house price)\n",
        "X = np.array([[1400], [1600], [1700], [1875], [1100], [1550], [2350], [2450], [1425], [1700]])\n",
        "Y = np.array([245000, 312000, 279000, 308000, 199000, 219000, 405000, 324000, 319000, 255000])\n",
        "\n",
        "# Split the data into training and trsting sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "\n",
        "# Alpha controls the regularization strength\n",
        "ridge_model.fit(X_train, Y_train)\n",
        "ridge_pred = ridge_model.predict(X_test)\n",
        "ridge_mse = mean_squared_error(Y_test, ridge_pred)\n",
        "print(\"Ridge Mean Squared Error:\", ridge_mse)\n",
        "\n",
        "# Lasso Regression\n",
        "lasso_model = Lasso(alpha=0.1)\n",
        "# Alpha controls the regularization strength\n",
        "lasso_model.fit(X_train, Y_train)\n",
        "lasso_pred = lasso_model.predict(X_test)\n",
        "lasso_mse = mean_squared_error(Y_test, lasso_pred)\n",
        "print(\"Lasso Mean Squared Error:\", lasso_mse)"
      ]
    }
  ]
}